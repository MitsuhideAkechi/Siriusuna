专门应用于影像

![](IMG-20251210185550202.png)

# The Shortage of Fully Connected Network
![](IMG-20251210185550344.png)
参数过多，容易过拟合

# 影像的特点
## 通过辨别某些特征识别物体
![](IMG-20251210185550375.png)
## 同一个 Pattern 可能出现在图片不同区域
因此如果每个 Receptive field 都放置所有 Pattern 对应的识别神经元，参数太多太浪费
![](IMG-20251210185550488.png)
## 下采样不会改变物体
![](IMG-20251210185550521.png)
# 做出的简化
## Receptive Field
![](IMG-20251210185550552.png)
Receptive field 很多形态、分布特征都可以自己设计，根据需要。
![](IMG-20251210185550673.png)
_**经典设计方案：**_
![](IMG-20251210185550706.png)

## Parameter Sharing
![](IMG-20251210185550855.png)
![](IMG-20251210185550990.png)
给守备不同区域的同种神经元共享参数

_**经典设计方案：**_
![](IMG-20251210185551023.png)

## Pooling
没有权重参数等要训练，类似于 Activation Function，做一些运算
![](IMG-20251210185551055.png)
![](IMG-20251210185551087.png)
池化核大小、池化方法，都可以根据需求调整

![](IMG-20251210185551223.png)
与卷积层交替使用，但主要目的是减少运算量，对于精细结构容易丢失 Pattern 特征，越来越多的模型丢弃了这一步。
## 小结
![](IMG-20251210185551254.png)
以上两个改进，形成的叫做卷积层，Model Bias 更大，但是专为影像设计，在影像上不容易过拟合，更专精

### 另一种视角
![](IMG-20251210185551285.png)
![](IMG-20251210185551397.png)
![](IMG-20251210185551538.png)
![](IMG-20251210185551566.png)
![](IMG-20251210185551701.png)
输出越大，越符合 Pattern（符合 Pattern 的地方对应正权值，不符合的地方对应负权值）
![](IMG-20251210185551735.png)
![](IMG-20251210185551766.png)

_**Multiple Convolutional Layers:**_
![](IMG-20251210185551795.png)
![](IMG-20251210185551917.png)
这样叠加层数，可以让我们的 filter 看到更大的范围。这也是前面提到的 kernel size 一般为 3 的原因，因为可以加层数看大范围，没必要加大核大小。  

当然，filters 还是有 bias 的，只是上文未提及

### 两种视角对比
![](IMG-20251210185551948.png)
![](IMG-20251210185551976.png)
![](IMG-20251210185552087.png)

# Summary
![](IMG-20251210185552249.png)

# Application: Go
![](IMG-20251210185552281.png)
## Why Use CNN?
![](IMG-20251210185552417.png)
![](IMG-20251210185552449.png)
要用什么方法，没有银弹，不能墨守成规！要根据实际问题分析

# The Limitation of CNN
![](IMG-20251210185552478.png)
